///////////////////////////////
// Global coral atlas project - Southeastern Asia
// Contact: mitchell.lyons@gmail.com
// Region coordinator: Eva Kovacs (kovacsevam2@gmail.com)
// Description:
// - Developing a process to combine OBIA and supervised classification
// - This script loads the pixel-based and segmentation data and training data and exports the training sampled
// - Corresponding '_datagen' script performs the data gathering/segmentation
// - Corresponding '_classification' script performs the classification 
///////////////////////////////

// Table of contents
// 1. Setting constants
// 2. Data loads & vis
// 4. Create training data
// 5. Export data

// Load and libraries needed
var map_palettes = require('users/mitchest/global_reefs_modules:colour_pals');
var param_module = require('users/mitchest/global_reefs_modules:reef_params');

// ###########################################
// SENSOR GENERICS
var sensor_params = param_module.dove;         //<------------ THIS IS WHERE YOU CHOOSE THE SENSOR
// REGION AND SENSOR SPECIFIC LOAD PATHS
var region_params = param_module.tropical_eastern_pacific;           //<------------ THIS IS WHERE YOU CHOOSE THE REGION
//  ^^ all the data paths are in this module ^^
// ###########################################

// 1. Setting constants

// These will get written to the asset metadata 

var vars = {
 
  // analysis type
  geomorphic: true, // map geomorphic zonation (when set to true) or benthic habitat (when set to false)
  use_benthic_points: false, // true = use benthic field points instead of map
  
  // analysis parameters
  image_data_scale: sensor_params.pixel, // pixel size of the image data
  class_field: 'class_num', // the field in the training data that stores the class integer
  slope_fix: true, //slope is tempermental for noisy depth products - if this is true, slope is unamsked to 0 - DON'T USE SLOPE IN CLASSIFICATION IN THIS INSTANCE
  
  // class information for classification, sampling and reporting
  train_size: 2000,
  local_epsg: region_params.epsg,
  // trim input data
  trim_training_data: false, // if true, will trim the training data set by the % below
  trim_training_perc: 0.60,
  classes_mapped_geo: [2, 11, 12,  13,   14,   15,  16,  21,  22,  23,  24], // classes to map (that is, to sample)
  classes_mapped_benthic: [11,   12,   13,   14,   15,   18], // classes to map (that is, to sample)
};


// 2. Data loads & vis

/*
make the combined pixels and segemnts data
 - test if there's more than one image asset to make up the region, then construct appropriately

*/
if (ee.List(region_params.pixels).length().getInfo() > 1) {
  print("Combining pixels/segs from 2 parts");
  var pixels = ee.Image(region_params.pixels[0]).unmask(0, false)
          .add(ee.Image(region_params.pixels[1]).unmask(0,false))
          .selfMask();
  } else {
    var pixels = ee.Image(region_params.pixels);
  }
  
if (ee.List(region_params.segments).length().getInfo() > 1) {
  print("Combining pixels/segs from 2 parts");
  var segments = ee.Image(region_params.segments[0]).unmask(0, false)
            .add(ee.Image(region_params.segments[1]).unmask(0,false))
            .selfMask();
  } else {
    var segments = ee.Image(region_params.segments);
  }

var pixsegs = pixels.regexpRename('(^.*$)','$1_p').addBands(
              segments.regexpRename('(^.*$)','$1_s'));


// fix dodgy slope calculations
if (vars.slope_fix) {
  pixsegs = pixsegs.addBands({
    srcImg: ee.Image([pixsegs.select('slope_p').unmask(0),pixsegs.select('slope_s').unmask(0)]).updateMask(pixsegs.select('depth_p')),
    overwrite: true});
}

// training data
var geomorph = ee.FeatureCollection(region_params.geo_train);
var benthic = ee.FeatureCollection(region_params.benthic_train);
if (vars.use_benthic_points) var benthic_pts = ee.FeatureCollection(region_params.benthic_pts);

// Add the raw data
var all_training_data = (vars.geomorphic) ? geomorph : benthic; // Set the training points to either geomorphic or benthic
// trim the data (randomly) if needed
if (vars.trim_training_data) {
  all_training_data = all_training_data.randomColumn("random");
  all_training_data = all_training_data.filter(ee.Filter.lte("random", vars.trim_training_perc));
}

Map.centerObject(pixsegs, 6);
Map.addLayer(pixsegs, {}, 'segments + pixels', false);
Map.addLayer(all_training_data, {}, "Training polygons", false);



// 4. Create training data

// set the class information
var classes_mapped = (vars.geomorphic) ? vars.classes_mapped_geo : vars.classes_mapped_benthic;

var sample_class_n = function(classn) {
    var target_features = all_training_data.filterMetadata(vars.class_field, 'equals', classn).geometry();
    var target_pts = ee.FeatureCollection.randomPoints(target_features, vars.train_size, samp_seed)
                      .map(function(f){
                        return(f.set(vars.class_field, classn));
                      });
    
    return(target_pts);
  };

if (!vars.geomorphic && vars.use_benthic_points) {
  // draw trainign/validation points from cal/val map + input point data set
  // cal points
  var samp_seed = 42;
  var map_pts_cal = ee.FeatureCollection(classes_mapped.map(sample_class_n)).flatten();
  var train_pts_cal = benthic_pts.merge(map_pts); // use benthic field data poitns to train a map
  // val_points
  samp_seed = 666;
  var map_pts_val = ee.FeatureCollection(classes_mapped.map(sample_class_n)).flatten();
  var train_pts_val = benthic_pts.merge(map_pts); // use benthic field data poitns to train a map
  
} else {
  // draw trainign/validation points from cal/val map only
  // cal points
  var samp_seed = 42;
  var train_pts_cal = ee.FeatureCollection(classes_mapped.map(sample_class_n)).flatten();
  // val_points
  samp_seed = 666;
  var train_pts_val = ee.FeatureCollection(classes_mapped.map(sample_class_n)).flatten();

}


// sample the predictor data
var training_data = pixsegs.sampleRegions({
    collection: train_pts_cal,
    properties: [vars.class_field],
    scale: vars.image_data_scale,
    geometries: true
});



// 7. Export data
var cal_name = (vars.geomorphic) ? region_params.asset + 'training/' + region_params.sname + '_' + 'geo_training' : region_params.asset + 'training/' + region_params.sname + '_' + 'benthic_training';
var cal_desc = (vars.geomorphic) ? region_params.sname + '_geo_training' : region_params.sname + '_benthic_training';
var val_name = (vars.geomorphic) ? region_params.asset + 'training/' + region_params.sname + '_' + 'geo_validation' : region_params.asset + 'training/' + region_params.sname + '_' + 'benthic_validation';
var val_desc = (vars.geomorphic) ? region_params.sname + '_geo_validation' : region_params.sname + '_benthic_validation';

Export.table.toAsset({
  collection: training_data, 
  description:  cal_desc, 
  assetId: cal_name
  });

Export.table.toAsset({
  collection: train_pts_val, 
  description:  val_desc, 
  assetId: val_name
});


// last so it sits on top
Map.addLayer(train_pts_cal.geometry(), {}, "Training point distribution", false); // Training data points
Map.addLayer(train_pts_val.geometry(), {}, "Validation point distribution", false); // Validation data points
if (vars.use_benthic_points) Map.addLayer(benthic_pts, {}, "Training/validation field point distribution", false); // Training data points