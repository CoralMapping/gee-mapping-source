/**** Start of imports. If edited, may not auto-convert in the playground. ****/
var validation = 
    /* color: #2000d6 */
    /* displayProperties: [
      {
        "type": "rectangle"
      }
    ] */
    ee.Geometry.Polygon(
        [[[176.20146869140808, -15.610233009654474],
          [176.20146869140808, -19.503902652781576],
          [182.36891864257973, -19.503902652781576],
          [182.36891864257973, -15.610233009654474]]], null, false),
    test_val = 
    /* color: #d63000 */
    /* displayProperties: [
      {
        "type": "rectangle"
      }
    ] */
    ee.Geometry.Polygon(
        [[[177.84943243669204, -18.77407437643519],
          [177.84943243669204, -19.32446074215668],
          [178.70636603044204, -19.32446074215668],
          [178.70636603044204, -18.77407437643519]]], null, false);
/***** End of imports. If edited, may not auto-convert in the playground. *****/
/*
############### --> need to port this over form previous GBR work

TODO (fixing up this script):

- Will this have lots of variation per region?
  - if yes, then have validation module per region
  - if no, then we can have a central and generic script that loads data via modules only
    - in that case we would need the "validation" geometry to be an asset and loaded via reef_params module
      
- Complete export module (i.e. export error mats to .csv)

- Implement options for sub-sampling the validation library (srs, spatial, block, hold-out etc.)


######
BIG COMPONENT TODO (new implementation required)
######
- Figure out how to do the 'confidence maps'
    - resmapling based?
    - class based?
    - distance to field data based?
    - hybrid of all above?
######
######
*/


///////////////////////////////
// Global coral atlas project - South West Pacific Region
// Contact: mitchell.lyons@gmail.com
// Description:
// - Developing a process to combine OBIA and supervised classification
// - This script loads the cleaned up classification and assesses accuracy
// - Corresponding '_calval', '_classification' and '_cleanup' script performs the data gathering/segmentation
///////////////////////////////

// Table of contents
// 1. Setting constants
// 2. Data loads & vis
// 3. Accuracy assessment
// 4. Export data
// 5. User accuracy-based confidence layer
// 6. Legend 

// Load and libraries needed
var map_palettes = require('users/mitchest/global_reefs:Modules/colour_pals')
var param_module = require('users/mitchest/global_reefs:Modules/reef_params')



// ###########################################
// SENSOR GENERICS
var sensor_params = param_module.dove         //<------------ THIS IS WHERE YOU CHOOSE THE SENSOR
// REGION AND SENSOR SPECIFIC LOAD PATHS
var region_params = param_module.swpac  //<------------ THIS IS WHERE YOU CHOOSE THE REGION
//  ^^ all the data paths are in this module ^^
// ###########################################

// 1. Setting constants

// These will get written to the asset metadata 

var vars = {
  
  // analysis type
  geomorphic: true, // assess geomorphic accuracy
  benthic: false, // assess benthic accuracy
  
  assess_clean: true, // assess the OBIA clean version or not (allows to to validation on early stage mapping)
  
  // analysis parameters
  image_data_scale: sensor_params.pixel, // pixel size of the image data
  use_benthic_points: false, // this flag is for using field photo transect points
  use_geo_points: false, // this flag is for using field photo transect points (also it is very unlikely we'll ever have geo points)
  
  // cal/val params
  val_trim: 0.1, // percentage at which to trim the validation library i.e. if 0.1 = 10% of vlaidation library, so @ 3000 points per class = 300 points per class
  val_trim_seed: 42,
  
  // class information for classification, sampling and reporting
  class_field: 'class_num', // the field in the training data that stores the class integer
  // TODO: consider dynamically generating these
  classes_mapped_geo:       [11,  13,   14,   15,  16,    21,  22], // classes to map
  classes_mapped_names_geo: ['SL','IRF','ORF','RR', 'TRF','SS','SE'],
  // TODO: consider dynamically generating these
  classes_mapped_benthic:       [11,    12,   13,  14,    16,   17,   18], // classes to map
  classes_mapped_names_benthic: ['SA', 'RU', 'RO', 'SG', 'CO', 'AL', 'BMA' ],
  
  // results/layers to show
  assess_accuracy: true, // do the accuracy assessment and write accuracy stats to output?
  show_accuracy: true, // print error stats to console? 
  
  // export options
  do_export: false, // export the results?
  export_scale: sensor_params.pixel, // pixel size to export at
  geomorph_output_name: region_params.sname + '_' + sensor_params.sname + '_geo-val',
  benthic_output_name: region_params.sname + '_' + sensor_params.sname + '_benthic-val',
  asset_output: region_params.asset, // asset path

}

// 2. Data loads & vis

// load input data
var pixels = ee.Image(region_params.dove_pixels)
var segments = ee.Image(region_params.dove_segments)
var geomorph = (vars.use_geo_points) ? ee.FeatureCollection(region_params.geo_pts) : ee.FeatureCollection(region_params.geo_val_library)
var benthic = (vars.use_benthic_points) ? ee.FeatureCollection(region_params.benthic_pts) : ee.FeatureCollection(region_params.benthic_val_library)
if (vars.assess_clean) {
  var geo_map = ee.Image(region_params.geo_map_clean)
  var benthic_map = ee.Image(region_params.benthic_map_clean)  
} else {
  var geo_map = ee.ImageCollection(region_params.geo_map).mosaic()
  var benthic_map = ee.ImageCollection(region_params.benthic_map).mosaic()
}

// Add the raw data
var all_training_data = (vars.geomorphic) ? geomorph : benthic // Set the training points to either geomorphic or benthic
var val_map = (vars.geomorphic) ? geo_map : benthic_map // Set the validation map to either geomorphic or benthic
var val_mapVIS = (vars.geomorphic) ? map_palettes.geo : map_palettes.benthic //Set the map visualization parameters to either geomorphic or benthic 
// set the class information
var nosamp_class = (vars.geomorphic) ? vars.nosamp_class_geo : vars.nosamp_class_benthic
var nosamp_numpix = (vars.geomorphic) ? vars.nosamp_numpix_geo : vars.nosamp_numpix_benthic
var classes_mapped = (vars.geomorphic) ? vars.classes_mapped_geo : vars.classes_mapped_benthic
var classes_mapped_names = (vars.geomorphic) ? vars.classes_mapped_names_geo : vars.classes_mapped_names_benthic


// 3. Accuracy assessment

if (vars.assess_accuracy) {
  
  if (vars.benthic && vars.use_benthic_points) { 
    
    var validation_pts = all_training_data // if we're doign a point based assessment (from field data) then use those
    
  } else if (vars.geomorphic && vars.use_geo_points) {
    
    var validation_pts = all_training_data // if we're doign a point based assessment (from field data) then use those
    
  } else {
    
    // otherwise use the cal/val library and sub-sample it somehow
    
    // here we just randomly sample it down, but could do a range of different things here
    
    var validation_pts = all_training_data.randomColumn({seed: vars.val_trim_seed}).filterMetadata('random', 'less_than', vars.val_trim)
    
  }
  
  // calculate an error matrix - won't account for unmapped areas though
  var accuracy_collection = val_map.unmask()
        .reduceRegions({
          collection: validation_pts,
          reducer: ee.Reducer.first(),
          scale: sensor_params.pixel,
        })
  
  // make the error matrix, and include order of classes
  var classification_errormatrix = accuracy_collection.errorMatrix({
    actual: vars.class_field, 
    predicted: 'first',
    order: classes_mapped, 
    })
  
  // set the accuracy results into the vars dictionary to be written into the file
  var accuracy_oa = classification_errormatrix.accuracy()
  var accuracy_cons = ee.Dictionary.fromLists({
    keys: classes_mapped_names,
    values: classification_errormatrix.consumersAccuracy().toList().flatten()
  })
  var accuracy_prod = ee.Dictionary.fromLists({
    keys: classes_mapped_names,
    values: classification_errormatrix.producersAccuracy().toList().flatten()
  })
  if (vars.geomorphic) {print ('Assessment of Accuracy for Geomorphic layer')} else {print('Assessment of Accuracy for Benthic layer')}
  if (vars.show_accuracy) {
    print("Overall accuracy", accuracy_oa)
    print("User's accuracy", accuracy_cons)
    print("Producer's accuracy", accuracy_prod)
    
    var classification_errormatrix_array = classification_errormatrix.array()
    // make a table we can use to interpret mis-classification
    var array_to_datatable = function(array) {
      var class_names = ee.List(classes_mapped_names)//.aside(print)
      // function to iterate over class names and return Google vis DataTable columns
      function toTableColumns(s) {
        return {id: s, label: s, type: 'number'} 
      }
      var columns = class_names.map(toTableColumns)//.aside(print,'columns')
      // function to iterate over array and return Google vis DataTable rows
      function featureToTableRow(f) {
        return {c: ee.List(f).map(function(c) { return {v: c} })}
      }
      var rows = array.toList().map(featureToTableRow)
      // dictionary to pass to chart ui
      return ee.Dictionary({cols: columns, rows: rows})
    }
    
    var dataTable = array_to_datatable(classification_errormatrix_array)
                            .evaluate(function(dataTable) {
                                // weird and i don't understand why we evaluate like this, but hey, it doesn't work outside
                                print('------------- Error matrix -------------',
                                ui.Chart(dataTable, 'Table').setOptions({pageSize: 15}),
                                'rows: reference, cols: mapped')
                            })
  }
} 



// 4. Export

// export to .csv?
// or join to the input classification assset and re-write with accuracy in metadata?
// or both?

//.set("overall_acc", accuracy_oa, "user_acc", accuracy_cons, "producer_acc", accuracy_prod)




//5. user accuracy-based confidence layer
Map.addLayer(val_map,val_mapVIS,'val_map');
Map.centerObject(validation,9);


var values_UA=(accuracy_cons.values());
var UAlayer=val_map.remap(ee.List(classes_mapped),values_UA);//.aside(print)
Map.addLayer(UAlayer,map_palettes.UA_pal,'UAlayer');

function wide(img){
  var wideIm=img.expression(
    
      "(b('remapped') >= 0.9) ? 6"+
      ":(b('remapped') >= 0.8) ? 5"+
        ":(b('remapped') >= 0.7) ? 4"+
          ":(b('remapped') >= 0.6) ? 3"+
            ":(b('remapped') >= 0.5) ? 2"+
              ":1"
  );
return wideIm;
}

function highConf(img){
  var highConfIm=img.expression(
    "(b('remapped') >= 0.98) ? 6"+
      ":(b('remapped') >= 0.96) ? 5"+
        ":(b('remapped') >= 0.94) ? 4"+
          ":(b('remapped') >= 0.92) ? 3"+
            ":(b('remapped') >= 0.90) ? 2"+
              ":1"
  );
return highConfIm;
}

var listmin=values_UA.reduce(ee.Reducer.min())//.aside(print,'listimin');
var reclass=ee.Image(ee.Algorithms.If(ee.Number(listmin).lt(0.6), wide(UAlayer), highConf(UAlayer)))//.aside(print);

function maskUA (img) {
  var mask=(UAlayer.multiply(0)).eq(0);
  return img.updateMask(mask);
}

var UserAcc=maskUA(reclass);


var toMap=UserAcc

Map.addLayer(toMap,map_palettes.UserAcc_pal,'UserAcc');
//print('toMap',toMap);

//6. legend  
//based t GEE example: https://code.earthengine.google.com/c507af58b49f5467a04449837edc8337
var legend=ui.Panel({
  style: {
    position: 'bottom-left',
    padding: '8px 15px'
  }
});

if (vars.geomorphic) {var Value='User Accuracy Geomorphic (%)'} else {var Value='User Accuracy Benthic (%)'}
var legendTitle=ui.Label({
  value: Value,
  style: {
    fontWeight:'bold',
    fontSize:'18px',
    margin:'0 0 4px 0',
    padding: '0'
  }
});
legend.add(legendTitle);


var dictH={
  '0-90':'f1eef6',
  '90-92':'d0d1e6',
  '92-94':'a6bddb',
  '94-96':'74a9cf',
  '96-98':'2b8cbe',
  '98-100':'045a8d'
};

var dictW={
  '0-50':'f1eef6',
  '50-60':'d0d1e6',
  '60-70':'a6bddb',
  '70-80':'74a9cf',
  '80-90':'2b8cbe',
  '90-100':'045a8d'
};
//print(dictW)

var Tcase=(ee.Number(listmin));
var boolean=(Tcase.getInfo() < 0.6);
//print('boolean',boolean);

if (boolean) {var dict=dictW} else {var dict=dictH}


var makeRow = function(color, name) {
  // Create the label that is actually the colored box.
  var colorBox = ui.Label({
    style: {
      backgroundColor: '#' + color,
      // Use padding to give the box height and width.
      padding: '9px',
      margin: '1 0 4px 0'
    }
  });

  // Create the label filled with the description text.
  var description = ui.Label({
    value: name,
    style: {margin: '0 0 4px 6px'}
  });

  return ui.Panel({
    widgets: [colorBox, description],
    layout: ui.Panel.Layout.Flow('horizontal')
  });
};

Object.keys(dict).map(function(key) {
  var value = dict[key];
  return legend.add(makeRow(value, key));
});

Map.add(legend);
