/*
TODO (fixing up this script):

- Will this have lots of variation per region?
  - if yes, then have validation module per region
  - if no, then we can have a central and generic script that loads data via modules only
    - in that case we would need the "validation" geometry to be an asset and loaded via reef_params module
      
- Complete export module (i.e. export error mats to .csv)

- Implement options for sub-sampling the validation library (srs, spatial, block, hold-out etc.)


######
BIG COMPONENT TODO (new implementation required)
######
- Figure out how to do the 'confidence maps'
    - resmapling based?
    - class based?
    - distance to field data based?
    - hybrid of all above?
######
######
*/


///////////////////////////////
// Global coral atlas project - South West Pacific Region
// Contact: mitchell.lyons@gmail.com
// Description:
// - Developing a process to combine OBIA and supervised classification
// - This script loads the cleaned up classification and assesses accuracy
// - Corresponding '_calval', '_classification' and '_cleanup' script performs the data gathering/segmentation
///////////////////////////////

// Table of contents
// 1. Setting constants
// 2. Data loads & vis
// 3. Accuracy assessment
// 4. Export data
// 5. User accuracy-based confidence layer
// 6. Legend 

// Load and libraries needed
var map_palettes = require('users/mitchest/global_reefs:Modules/colour_pals')
var param_module = require('users/mitchest/global_reefs:Modules/reef_params')
var pkg_vis = require('users/mitchest/global_reefs:Modules/pkg_vis')



// ###########################################
// SENSOR GENERICS
var sensor_params = param_module.dove         //<------------ THIS IS WHERE YOU CHOOSE THE SENSOR
// REGION AND SENSOR SPECIFIC LOAD PATHS
var region_params = param_module.swpac  //<------------ THIS IS WHERE YOU CHOOSE THE REGION
//  ^^ all the data paths are in this module ^^
// ###########################################

// 1. Setting constants

// These will get written to the asset metadata 

var vars = {
  
  // analysis type
  geomorphic: true, // assess geomorphic accuracy (one or the other)
  benthic: false, // assess benthic accuracy
  
  assess_clean: true, // assess the OBIA clean version or not (allows to to validation on early stage mapping)
  
  // analysis parameters
  image_data_scale: sensor_params.pixel, // pixel size of the image data
  use_benthic_points: false, // this flag is for using field photo transect points
  use_geo_points: false, // this flag is for using field photo transect points (also it is very unlikely we'll ever have geo points)
  
  // cal/val params
  val_trim: 0.1, // percentage at which to trim the validation library i.e. if 0.1 = 10% of vlaidation library, so @ 3000 points per class = 300 points per class
  val_trim_seed: 42,
  
  // class information for classification, sampling and reporting
  class_field: 'class_num', // the field in the training data that stores the class integer
  // TODO: consider dynamically generating these
  classes_mapped_geo:       [11,  13,   14,   15,  16,    21,  22], // classes to map
  classes_mapped_names_geo: ['SL','IRF','ORF','RR', 'TRF','SS','SE'],
  // TODO: consider dynamically generating these
  classes_mapped_benthic:       [11,    12,   13,  14,    16,   17,   18], // classes to map
  classes_mapped_names_benthic: ['SA', 'RU', 'RO', 'SG', 'CO', 'AL', 'BMA' ],
  
  // results/layers to show
  assess_accuracy: true, // do the accuracy assessment and write accuracy stats to output?
  show_accuracy: true, // print error stats to console? 
  
  // export options
  do_export: false, // export the results?
  export_scale: sensor_params.pixel, // pixel size to export at
  geomorph_output_name: region_params.sname + '_' + sensor_params.sname + '_geo-val',
  benthic_output_name: region_params.sname + '_' + sensor_params.sname + '_benthic-val',
  asset_output: region_params.asset, // asset path

}

// 2. Data loads & vis

// load input data
var pixels = ee.Image(region_params.dove_pixels)
var segments = ee.Image(region_params.dove_segments)
var geomorph = (vars.use_geo_points) ? ee.FeatureCollection(region_params.geo_pts) : ee.FeatureCollection(region_params.geo_val_library)
var benthic = (vars.use_benthic_points) ? ee.FeatureCollection(region_params.benthic_pts) : ee.FeatureCollection(region_params.benthic_val_library)
if (vars.assess_clean) {
  var geo_map = ee.Image(region_params.geo_map_clean)
  var benthic_map = ee.Image(region_params.benthic_map_clean)  
} else {
  var geo_map = ee.ImageCollection(region_params.geo_map).mosaic()
  var benthic_map = ee.ImageCollection(region_params.benthic_map).mosaic()
}

// Add the raw data
var all_training_data0 = (vars.geomorphic) ? geomorph : benthic // Set the training points to either geomorphic or benthic
var val_map = (vars.geomorphic) ? geo_map : benthic_map // Set the validation map to either geomorphic or benthic
var val_mapVIS = (vars.geomorphic) ? map_palettes.geo : map_palettes.benthic //Set the map visualization parameters to either geomorphic or benthic 

// set the class information
var nosamp_class = (vars.geomorphic) ? vars.nosamp_class_geo : vars.nosamp_class_benthic;
var nosamp_numpix = (vars.geomorphic) ? vars.nosamp_numpix_geo : vars.nosamp_numpix_benthic;
var classes_mapped = (vars.geomorphic) ? vars.classes_mapped_geo : vars.classes_mapped_benthic;
var classes_mapped_names = (vars.geomorphic) ? vars.classes_mapped_names_geo : vars.classes_mapped_names_benthic;

//clip reference data according to 'classes_mapped'
var ref_points=ee.Filter.inList('class_num',classes_mapped);
var all_training_data=all_training_data0.filter(ref_points);

// 3. Accuracy assessment

if (vars.assess_accuracy) {
  
  if (vars.benthic && vars.use_benthic_points) { 
    
    var validation_pts = all_training_data // if we're doign a point based assessment (from field data) then use those
    
  } else if (vars.geomorphic && vars.use_geo_points) {
    
    var validation_pts = all_training_data // if we're doign a point based assessment (from field data) then use those
    
  } else {
    
    // otherwise use the cal/val library and sub-sample it somehow
    
    // here we just randomly sample it down, but could do a range of different things here
    
    var validation_pts = all_training_data.randomColumn({seed: vars.val_trim_seed}).filterMetadata('random', 'less_than', vars.val_trim)
    
  }
  
  Map.addLayer(validation_pts, {}, "validation points used", false)
  
  // calculate an error matrix - won't account for unmapped areas though
  var accuracy_collection = val_map.unmask()
        .reduceRegions({
          collection: validation_pts,
          reducer: ee.Reducer.first(),
          scale: sensor_params.pixel,
        })
  
  // make the error matrix, and include order of classes
  var classification_errormatrix = accuracy_collection.errorMatrix({
    actual: vars.class_field, 
    predicted: 'first',
    order: classes_mapped, 
    })
  
  // set the accuracy results into the vars dictionary to be written into the file
  var accuracy_oa = classification_errormatrix.accuracy()
  var accuracy_cons = ee.Dictionary.fromLists({
    keys: classes_mapped_names,
    values: classification_errormatrix.consumersAccuracy().toList().flatten()
  })
  var accuracy_prod = ee.Dictionary.fromLists({
    keys: classes_mapped_names,
    values: classification_errormatrix.producersAccuracy().toList().flatten()
  })
  if (vars.geomorphic) {print ('Assessment of Accuracy for Geomorphic layer')} else {print('Assessment of Accuracy for Benthic layer')}
  if (vars.show_accuracy) {
    print("Overall accuracy", accuracy_oa)
    print("User's accuracy", accuracy_cons)
    print("Producer's accuracy", accuracy_prod)
    
    var classification_errormatrix_array = classification_errormatrix.array()
    // make a table we can use to interpret mis-classification
    var array_to_datatable = function(array) {
      var class_names = ee.List(classes_mapped_names)//.aside(print)
      // function to iterate over class names and return Google vis DataTable columns
      function toTableColumns(s) {
        return {id: s, label: s, type: 'number'} 
      }
      var columns = class_names.map(toTableColumns)//.aside(print,'columns')
      // function to iterate over array and return Google vis DataTable rows
      function featureToTableRow(f) {
        return {c: ee.List(f).map(function(c) { return {v: c} })}
      }
      var rows = array.toList().map(featureToTableRow)
      // dictionary to pass to chart ui
      return ee.Dictionary({cols: columns, rows: rows})
    }
    
    var dataTable = array_to_datatable(classification_errormatrix_array)
                            .evaluate(function(dataTable) {
                                // weird and i don't understand why we evaluate like this, but hey, it doesn't work outside
                                print('------------- Error matrix -------------',
                                ui.Chart(dataTable, 'Table').setOptions({pageSize: 15}),
                                'rows: reference, cols: mapped')
                            })
  }
} 



// 4. Export

// export to .csv?
// or join to the input classification assset and re-write with accuracy in metadata?
// or both?

//.set("overall_acc", accuracy_oa, "user_acc", accuracy_cons, "producer_acc", accuracy_prod)




//5. user accuracy-based confidence layer
Map.addLayer(val_map, val_mapVIS, 'Valdiation map', false);
Map.centerObject(validation_pts,9);


var values_UA=(accuracy_cons.values());
var UAlayer=val_map.remap(ee.List(classes_mapped),values_UA);//.aside(print)
Map.addLayer(UAlayer, map_palettes.UA_pal, 'User Accuracy layer', false);

// function wide(img){
//   var wideIm=img.expression(
    
//       "(b('remapped') >= 0.9) ? 6"+
//       ":(b('remapped') >= 0.8) ? 5"+
//         ":(b('remapped') >= 0.7) ? 4"+
//           ":(b('remapped') >= 0.6) ? 3"+
//             ":(b('remapped') >= 0.5) ? 2"+
//               ":1"
//   );
// return wideIm;
// }

function highConf(img){
  var highConfIm=img.expression(
    "(b('remapped') >= 0.90) ? 7"+
      ":(b('remapped') >= 0.80) ? 6"+
        ":(b('remapped') >= 0.70) ? 5"+
          ":(b('remapped') >= 0.60) ? 4"+
            ":(b('remapped') >= 0.50) ? 3"+
              ":(b('remapped') >= 0.10) ? 2"+
                ":(b('remapped') >= 0.01) ? 1"+
                  ":0"
  );
return highConfIm;
}

// var listmin=values_UA.reduce(ee.Reducer.min())//.aside(print,'listimin');
// var reclass=ee.Image(ee.Algorithms.If(ee.Number(listmin).lt(0.6), wide(UAlayer), highConf(UAlayer)))//.aside(print);
var reclass=highConf(UAlayer);

function maskUA (img) {
  var mask=(UAlayer.multiply(0)).eq(0);
  return img.updateMask(mask);
}

var UserAcc=maskUA(reclass);


var toMap=UserAcc

Map.addLayer(toMap, map_palettes.UserAcc_pal, 'User Accuracy categories', false);
//print('toMap',toMap);

//6. legend  
//based t GEE example: https://code.earthengine.google.com/c507af58b49f5467a04449837edc8337

if (vars.geomorphic) {var title=ui.Label({
  value: 'User Accuracy Geomorphic (%)',
  style: {fontWeight: 'bold', fontSize: '18px'}
})} else {var title=ui.Label({
  value:'User Accuracy Benthic (%)',
  style: {fontWeight: 'bold', fontSize: '18px'}
})}

var legend=pkg_vis.discrete_legend(map_palettes.UserAcc_cat, map_palettes.UserAcc_cols, ' ',false);
pkg_vis.add_lgds([title,legend]);