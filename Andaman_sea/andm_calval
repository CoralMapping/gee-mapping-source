///////////////////////////////
// Global coral atlas project - Andaman sea + Myanmar
// Contact: mitchell.lyons@gmail.com
// Region coordinator: Eva Kovacs (...)
// Description:
// - Developing a process to combine OBIA and supervised classification
// - This script loads the pixel-based and segmentation data and training data and exports the training sampled
// - Corresponding '_datagen' script performs the data gathering/segmentation
// - Corresponding '_classification' script performs the classification 
///////////////////////////////

// Table of contents
// 1. Setting constants
// 2. Data loads & vis
// 4. Create training data
// 5. Export data

// Load and libraries needed
var map_palettes = require('users/mitchest/global_reefs:Modules/colour_pals')
var param_module = require('users/mitchest/global_reefs:Modules/reef_params')

// ###########################################
// SENSOR GENERICS
var sensor_params = param_module.dove         //<------------ THIS IS WHERE YOU CHOOSE THE SENSOR
// REGION AND SENSOR SPECIFIC LOAD PATHS
var region_params = param_module.andm          //<------------ THIS IS WHERE YOU CHOOSE THE REGION
//  ^^ all the data paths are in this module ^^
// ###########################################

// 1. Setting constants

// These will get written to the asset metadata 

var vars = {
  /* 
  - exporting training or validation data? This is done separately so when sampling from maps you use a DIFFERENT sample.
  Should be one of:
  'train' OR 'val'
  */
  export_type: 'train',
  
  // analysis type
  geomorphic: false, // map geomorphic zonation (when set to true) or benthic habitat (when set to false)
  use_benthic_points: false, // true = use benthic field points instead of map
  
  // analysis parameters
  image_data_scale: sensor_params.pixel, // pixel size of the image data
  class_field: 'class_num', // the field in the training data that stores the class integer
  slope_fix: true, //slope is tempermental for noisy depth products - if this is true, slope is unamsked to 0 - DON'T USE SLOPE IN CLASSIFICATION IN THIS INSTANCE
  
  // class information for classification, sampling and reporting
  train_size: 2000,
  local_epsg: region_params.epsg,
  // trim input data
  trim_training_data: false, // if true, will trim the training data set by the % below
  trim_training_perc: 0.60,
  // geomorphic
  nosamp_class_geo:  [0, 1, 3, 25, 26], // classes not to use/use more
  nosamp_numpix_geo: [0, 0, 0, 0,  0 ], // set number of pixels to sample from above classes
  // TODO: consider dynamically generating these
  //classes_mapped_geo:       [ 2,  11,  12,  13,   14,   15,  16,  21,  22,  23,  24  ], // classes to map
  //classes_mapped_names_geo: ['D','SL','DL','IRF','ORF','RR','TRF','SS','SE','PL','OCL'],
  // benthic
  nosamp_class_benthic:  [0, 1, 2, 3, 4], // classes not to use/use more
  nosamp_numpix_benthic: [0, 0, 0, 0, 0], // set number of pixels to sample from above classes
  // TODO: consider dynamically generating these
  //classes_mapped_benthic:       [ 3,    4,    11,   12,   13,   14,   15,   16,  17], // classes to map
  //classes_mapped_names_benthic: ['MAN','MU', 'SA', 'RU', 'RO', 'SG', 'CA', 'CO', 'AL'],

}


// 2. Data loads & vis

// load input data
/*var pixels = ee.ImageCollection(region_params.pixels).mosaic().regexpRename('(^.*$)','$1_p')
var segments = ee.ImageCollection(region_params.segments).mosaic().regexpRename('(^.*$)','$1_s')*/

/*
make the combined pixels and segemnts data
 - test if there's more than one image asset to make up the region, then construct appropriately
*/

if (ee.List(region_params.pixels).length().getInfo() > 1) {
  print("Combining pixels/segs from 2 parts")
  var pixels = ee.Image(region_params.pixels[0]).unmask(0, false)
          .add(ee.Image(region_params.pixels[1]).unmask(0,false))
          .selfMask()
  } else {
    var pixels = ee.Image(region_params.pixels)
  }
  
if (ee.List(region_params.segments).length().getInfo() > 1) {
  print("Combining pixels/segs from 2 parts")
  var segments = ee.Image(region_params.segments[0]).unmask(0, false)
            .add(ee.Image(region_params.segments[1]).unmask(0,false))
            .selfMask()
  } else {
    var segments = ee.Image(region_params.segments)
  }

var pixsegs = pixels.regexpRename('(^.*$)','$1_p').addBands(
              segments.regexpRename('(^.*$)','$1_s'))


// fix dodgy slope calculations
if (vars.slope_fix) {
  pixsegs = pixsegs.addBands({
    srcImg: ee.Image([pixsegs.select('slope_p').unmask(0),pixsegs.select('slope_s').unmask(0)]).updateMask(pixsegs.select('depth_p')),
    overwrite: true})
}

// training data
var geomorph = ee.FeatureCollection(region_params.geo_train)
var benthic = ee.FeatureCollection(region_params.benthic_train)
if (vars.use_benthic_points) var benthic_pts = ee.FeatureCollection(region_params.benthic_pts)

// Add the raw data
var all_training_data = (vars.geomorphic) ? geomorph : benthic // Set the training points to either geomorphic or benthic
// trim the data (randomly) if needed
if (vars.trim_training_data) {
  all_training_data = all_training_data.randomColumn("random")
  all_training_data = all_training_data.filter(ee.Filter.lte("random", vars.trim_training_perc))
}

// set the class information
var nosamp_class = (vars.geomorphic) ? vars.nosamp_class_geo : vars.nosamp_class_benthic
var nosamp_numpix = (vars.geomorphic) ? vars.nosamp_numpix_geo : vars.nosamp_numpix_benthic
/*var classes_mapped = (vars.geomorphic) ? vars.classes_mapped_geo : vars.classes_mapped_benthic
var classes_mapped_names = (vars.geomorphic) ? vars.classes_mapped_names_geo : vars.classes_mapped_names_benthic*/



//Map.centerObject(pixsegs, 11)

//Map.addLayer(pixels, {}, 'per-pixel image data', false)
//Map.addLayer(segments, {}, 'segmented image data', false)
Map.addLayer(pixsegs, {}, 'segments + pixels', false)


// 4. Create training data

Map.addLayer(all_training_data, {}, "Training polygons", false)

// print a plot to see if oyu need to adjust sampling strategy
var get_area = function(f) {
  return(f.set("area", f.area()))
}
print(
  "Area distribution of reference data:",
  "Need to up- or down-sample any classes?",
  ui.Chart.feature.byFeature(all_training_data.map(get_area), "glob_class", "area")
      .setChartType('ColumnChart')
)

// paint the trinaing data out to a raster
//var training_painting = ee.Image().byte().paint(all_training_data, vars.class_field).rename(vars.class_field)//.selfMask()

var samp_seed = (vars.export_type == 'train') ? ee.Number(42) : ee.Number(666) // the default values should mix up the seeds, but this guarantees it

if (!vars.geomorphic && vars.use_benthic_points) {
  // draw trainign/validation points from cal/val map + input point data set
  var map_pts = ee.Image().byte().paint(all_training_data, vars.class_field).rename(vars.class_field)//.selfMask()
                      .updateMask(pixsegs.select(1).gte(0))
                      .addBands(ee.Image.pixelLonLat())
                      .stratifiedSample({
                        numPoints: vars.train_size,
                        region: all_training_data, // region to sample points from
                        projection: vars.local_epsg,
                        scale: sensor_params.pixel,
                        classValues: nosamp_class,
                        classPoints: nosamp_numpix,
                        seed: samp_seed
                      }).map(function(f) {
                        return f.setGeometry(ee.Geometry.Point([f.get('longitude'), f.get('latitude')]))
                      })
  var train_pts = benthic_pts.merge(map_pts) // use benthic field data poitns to train a map
  
} else {
  // draw trainign/validation points from cal/val map
  var train_pts = ee.Image().byte().paint(all_training_data, vars.class_field).rename(vars.class_field)//.selfMask()
                      .updateMask(pixsegs.select(1).gte(0))
                      .addBands(ee.Image.pixelLonLat())
                      .stratifiedSample({
                        numPoints: vars.train_size,
                        region: all_training_data, // region to sample points from
                        projection: vars.local_epsg,
                        scale: sensor_params.pixel,
                        classValues: nosamp_class,
                        classPoints: nosamp_numpix,
                        seed: samp_seed
                      }).map(function(f) {
                        return f.setGeometry(ee.Geometry.Point([f.get('longitude'), f.get('latitude')]))
                      })
}
// ## If you have training points from multiple UTM zones (and the local epsg is a UTM crs),
// ## then you need to run the above block within each zone and then merge.
// ## See 'gbr_classification' script for an example
//var train_pts = train_pts_cc.merge(train_pts_cbg) // merge to final set

/*
// function to create a X m buffer around the input training data (this may change when segments are used)
var buff_points = function (feature) {
  return feature.buffer(vars.train_point_buff_dist) // should get the 3x3 Landsat neighbourhood
}
var train_pts = train_pts.map(buff_points)
*/

// extract image data for classifier training
/*if (vars.pixels_or_objects == 'pixels') {
  var training_data = pixels.sampleRegions({
    collection: train_pts,
    properties: [vars.class_field],
    scale: vars.image_data_scale
  })
} else if (vars.pixels_or_objects == 'objects') {
  var training_data = segments.sampleRegions({
    collection: train_pts,
    properties: [vars.class_field],
    scale: vars.image_data_scale
  })
} else {
  var training_data = pixsegs.sampleRegions({
    collection: train_pts,
    properties: [vars.class_field],
    scale: vars.image_data_scale,
    geometries: false
  })
}*/

var training_data = pixsegs.sampleRegions({
    collection: train_pts,
    properties: [vars.class_field],
    scale: vars.image_data_scale,
    geometries: true
})


//print(benthic_pts.size())
//print(ignore_pts.size())
//print(train_pts.size())
//print(training_data.limit(1))



// 7. Export data
var cal_name = (vars.geomorphic) ? region_params.asset + 'training/' + region_params.sname + '_' + 'geo_training' : region_params.asset + 'training/' + region_params.sname + '_' + 'benthic_training'
var cal_desc = (vars.geomorphic) ? region_params.sname + '_geo_training' : region_params.sname + '_benthic_training'
var val_name = (vars.geomorphic) ? region_params.asset + 'training/' + region_params.sname + '_' + 'geo_validation' : region_params.asset + 'training/' + region_params.sname + '_' + 'benthic_validation'
var val_desc = (vars.geomorphic) ? region_params.sname + '_geo_validation' : region_params.sname + '_benthic_validation'

if (vars.export_type == 'train') {
  Export.table.toAsset({
    collection: training_data, 
    description:  cal_desc, 
    assetId: cal_name
  })
}

if (vars.export_type == 'val') {
  Export.table.toAsset({
    collection: train_pts, 
    description:  val_desc, 
    assetId: val_name
  })
}

// last so it sits on top
Map.addLayer(train_pts, {}, "Training/validation point distribution", false) // Training data points
if (vars.use_benthic_points) Map.addLayer(benthic_pts, {}, "Training/validation field point distribution", false) // Training data points
Map.addLayer(training_data, {}, "Image reference data distribution", false) // Training data points